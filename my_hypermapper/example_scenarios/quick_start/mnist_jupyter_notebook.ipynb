{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make HyperMapper solve Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, yay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "\n",
    "#tds\n",
    "import torchvision\n",
    "\n",
    "#git\n",
    "#from __future__ import print_function\n",
    "#import argparse\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads the MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had issues with batch size because torch.utils only accepts batch size in type int \n",
    "# and not numpy.int64 (standard type for integers in numpy.array()).\n",
    "# This I think is poor design but easy to work around\n",
    "def get_mnist(b_size = 64):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "      torchvision.datasets.MNIST('data/', train=True, \n",
    "                                  download=True,\n",
    "                                  transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                     (0.1307,), (0.3081,))\n",
    "                                 ])),\n",
    "                                  batch_size = b_size, \n",
    "                                    shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "      torchvision.datasets.MNIST('data/', train=False, download=True,\n",
    "                                 transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize(\n",
    "                                     (0.1307,), (0.3081,))\n",
    "                                 ])),\n",
    "                                  batch_size = 1000, shuffle=True)\n",
    "    # print('loaded the mnist data')\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translates HyperMapper json to pyTorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class json2pheno(nn.Module):\n",
    "    def __init__(self, json, nin, nout):\n",
    "        super(json2pheno, self).__init__()\n",
    "        # build layers from genome encoding\n",
    "\n",
    "        n_in = nin\n",
    "        fw_map = {}\n",
    "\n",
    "        \"\"\" What does json contain that we want here:\n",
    "        n_nodes\n",
    "        n_layers (All layers accept the output layer)\n",
    "        skip (length of skip connection. may be 0)\n",
    "        \"\"\" \n",
    "        n_nodes = json['n_nodes']\n",
    "        n_layers = json['n_layers']\n",
    "        skip = json['skip']\n",
    "        if 'activation' in json:\n",
    "            self.activation = json['activation']\n",
    "        else:\n",
    "            self.activation = 0\n",
    "        \n",
    "        # check so activation function is right\n",
    "        if self.activation == 0:\n",
    "             print('activation: tanh')\n",
    "        elif self.activation == 1:\n",
    "             print('activation: relu')\n",
    "        else:\n",
    "             print('error in act func')\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            key = str(i)\n",
    "            setattr(self, key, nn.Linear(n_in, n_nodes))\n",
    "           \n",
    "            # We are on the last hidden layer, so we will not have any skipps here\n",
    "            if i == n_layers - 1:\n",
    "                fw_map[key] = ['out']\n",
    "                n_in = n_nodes\n",
    "                break\n",
    "            \n",
    "            fw_map[key] = [str(i + 1)]\n",
    "            \n",
    "            # Add skips to the fw_map. If they are to long, sent them to output layer\n",
    "            if skip:\n",
    "                if i + skip + 1 < n_layers:\n",
    "                    fw_map[key].append(str(i + skip + 1))\n",
    "                else:\n",
    "                    fw_map[key].append('out')\n",
    "\n",
    "            \n",
    "            # Again, this is same for all but first layer\n",
    "            n_in = n_nodes\n",
    "\n",
    "        setattr(self, 'out', nn.Linear(n_in, nout))\n",
    "        \n",
    "        # fw_scheme is a dict containing to which layers each layer is sending its output\n",
    "        # This will fail if we have non-forward connections\n",
    "        self.fw_scheme = fw_map\n",
    "        print(self.fw_scheme)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = 0\n",
    "        X = dict()\n",
    "        X['0'] = [x.view(x.shape[0], -1)]\n",
    "        while hasattr(self, str(k)):\n",
    "            # pass trough all layers except the output layer\n",
    "            key = str(k)\n",
    "\n",
    "            # we might want to concat instead of sum, then we need to modify input_size in __init__\n",
    "            temp_x = sum(X[key])\n",
    "            # temp_out = torch.tanh(getattr(self, key)(temp_x))\n",
    "            if self.activation == 0:\n",
    "                temp_out = torch.tanh(getattr(self, key)(temp_x))\n",
    "            elif self.activation == 1:\n",
    "                temp_out = torch.relu(getattr(self, key)(temp_x))\n",
    "            else:\n",
    "                temp_out = getattr(self, key)(temp_x)\n",
    "                \n",
    "            # this seem to work when doing the list thing with x\n",
    "            for target in self.fw_scheme[key]:\n",
    "                if target in X:\n",
    "                    X[target].append(temp_out)\n",
    "                else:\n",
    "                    X[target] = [temp_out]\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        # if k = 0 we have no active layers and a perceptron model\n",
    "        if k:\n",
    "            temp_x = sum(X['out'])\n",
    "        else:\n",
    "            temp_x = x.view(x.shape[0],-1)\n",
    "\n",
    "        # Softmax as we are dealing with multiclass clasification problems\n",
    "        out = getattr(self, 'out')(temp_x)\n",
    "        return F.log_softmax(out, dim = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a json scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scenario(RS, BO=0, output_path = \"output_samples.csv\"):\n",
    "    scenario = {}\n",
    "    scenario[\"application_name\"] = \"mnist\"\n",
    "    scenario[\"optimization_objectives\"] = [\"Value\"]\n",
    "    scenario[\"input_parameters\"] = {}\n",
    "    scenario[\"output_data_file\"] = output_path\n",
    "    scenario[\"design_of_experiment\"] = {}\n",
    "\n",
    "    # doe: nbr of random samples\n",
    "    # BO: nbr op BO iters. BO = 0 means only random sampling\n",
    "    scenario[\"optimization_iterations\"] = BO\n",
    "    scenario[\"design_of_experiment\"][\"doe_type\"] = \"random sampling\"\n",
    "    scenario[\"design_of_experiment\"][\"number_of_samples\"] = RS\n",
    "\n",
    "    # number of units in the hidden layers\n",
    "    n_nodes = {}\n",
    "    n_nodes[\"parameter_type\"] = \"ordinal\"\n",
    "    n_nodes[\"values\"] = [2**i for i in range(2, int(math.log(128, 2)) + 1)] # 16-128\n",
    "\n",
    "    # number of layers, excluding output layer\n",
    "    n_layers = {}\n",
    "    n_layers[\"parameter_type\"] = \"ordinal\"\n",
    "    n_layers[\"values\"] = [0, 1, 2]\n",
    "\n",
    "    # Length of skip connections\n",
    "    # I do this ordinal because I feel like integer parameters most oftenly sets to zero.\n",
    "    skip = {}\n",
    "    skip[\"parameter_type\"] = \"ordinal\"\n",
    "    skip[\"values\"] = [0, 1]\n",
    "    \n",
    "    # Activation functions for hidden layers\n",
    "    # I do this ordinal because when I did categorical I still got ints when unpacking\n",
    "    activation = {}\n",
    "    activation[\"parameter_type\"] = \"ordinal\"\n",
    "    activation[\"values\"] = [0, 1]      # 0 = tanh, 1 = relu\n",
    "    \n",
    "    batch_size = {}\n",
    "    batch_size[\"parameter_type\"] = \"ordinal\"\n",
    "    batch_size[\"values\"] = [16, 32, 64, 128, 256]\n",
    "    \n",
    "    optimizer = {}\n",
    "    optimizer[\"parameter_type\"] = \"ordinal\"\n",
    "    optimizer[\"values\"] = [0, 1, 2]    # 0 = SGD, 1 = Adam, 2 = RMSprop. Learningrate not spec.\n",
    "\n",
    "    scenario[\"input_parameters\"][\"n_nodes\"] = n_nodes\n",
    "    scenario[\"input_parameters\"][\"n_layers\"] = n_layers\n",
    "    scenario[\"input_parameters\"][\"skip\"] = skip\n",
    "    scenario[\"input_parameters\"][\"activation\"] = activation   # optional param. deafult = tanh\n",
    "    scenario[\"input_parameters\"][\"batch_size\"] = batch_size   # optional param. default = 64\n",
    "    scenario[\"input_parameters\"][\"optimizer\"] = optimizer     # optional param. default = SGD\n",
    "\n",
    "\n",
    "    %cd\n",
    "    %cd \"PycharmProjects/samuel_nas/my_hypermapper/example_scenarios/quick_start\"\n",
    "    with open(\"example_mnist_scenario.json\", \"w\") as scenario_file:\n",
    "        json.dump(scenario, scenario_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains a network and returns validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(network, train_data, test_data, optimizer = 0):\n",
    "    \n",
    "    # Always uses cross entropy as loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimization algorithm given by the json\n",
    "    if optimizer == 0:\n",
    "        print('optimizer: sgd')\n",
    "        # SGD requires lr to be set\n",
    "        optimizer = optim.SGD(network.parameters(), lr=0.1)\n",
    "    elif optimizer == 1:\n",
    "        print('optimizer: adam')\n",
    "        optimizer = optim.Adam(network.parameters())\n",
    "    elif optimizer == 2:\n",
    "        print('optimizer: rmsprop')\n",
    "        optimizer = optim.RMSprop(network.parameters())\n",
    "        \n",
    "    # Train for a given number of epochs (1)\n",
    "    t0 = time.perf_counter()\n",
    "    for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_data, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            #running_loss += loss.item()\n",
    "            #if i % 200 == 199:    # print every 200 mini-batches\n",
    "             #   print('[%d, %5d] loss: %.3f' %\n",
    "              #        (epoch + 1, i + 1, running_loss / 200))\n",
    "               # running_loss = 0.0\n",
    "\n",
    "    print('Finished Training it took ', (time.perf_counter() - t0)/60, ' minutes to train')\n",
    "\n",
    "    # Validates performance on unseen data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_data:\n",
    "            images, labels = data\n",
    "            outputs = network(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    #print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "     #   100 * correct / total))\n",
    "\n",
    "    return 1 - correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function for hypermapper to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_function(X):\n",
    "    \"\"\"\n",
    "    Compute the error rate on MNIST after training for a given time.\n",
    "    :param X: dictionary containing the hyperparameters describing a network.\n",
    "    :return: the validation performance of the network described by X\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sets batch size if given in json. Otherwise defaults to 64\n",
    "    # Gets issues with type when taken from array. Then it must be converted to int\n",
    "    if 'batch_size' in X:\n",
    "        batch_size = X['batch_size']\n",
    "        if type(batch_size) != int:\n",
    "            # print('not a pos int!! type: ', type(batch_size))\n",
    "            batch_size = int(batch_size)\n",
    "            #print('Now it should be int. type: ', type(batch_size))\n",
    "            \n",
    "        print(batch_size)\n",
    "        train_loader, test_loader = get_mnist(batch_size)\n",
    "    else:\n",
    "        print('batch size not given, defaults to 64')\n",
    "        train_loader, test_loader = get_mnist()\n",
    "    \n",
    "    # The input/output size of the network. \n",
    "    # for fully connected nets nin is nbr of pixels\n",
    "    # for CNN nin would be number of chanels\n",
    "    nin = 28**2\n",
    "    nout = 10\n",
    "    my_net = json2pheno(X, nin, nout)\n",
    "\n",
    "    # Specifies optimizer if given in json scenario.\n",
    "    if 'optimizer' in X:\n",
    "        optimizer = X['optimizer']\n",
    "        loss = trainer(my_net, train_loader, test_loader, optimizer)\n",
    "    else:\n",
    "        print('optimizer not given')\n",
    "        loss = trainer(my_net, train_loader, test_loader)\n",
    "    \n",
    "    print('error: ', loss)\n",
    "    print('\\n')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basically the main method. Optimizes the given function based on the given scenario. \n",
    "\n",
    "### Stores the results in op.cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/samuel\n",
      "/home/samuel/PycharmProjects/samuel_nas/my_hypermapper/example_scenarios/quick_start\n",
      "/home/samuel\n",
      "/home/samuel/PycharmProjects/samuel_nas/my_hypermapper/scripts\n",
      "/home/samuel/PycharmProjects/samuel_nas/my_hypermapper\n",
      "Design of experiment phase, number of doe samples = 10 .......\n",
      "batch size is a pos int, it should work\n",
      "16\n",
      "activation: tanh\n",
      "{'0': ['out']}\n",
      "optimizer: rmsprop\n",
      "Finished Training it took  0.17640323741664662  minutes to train\n",
      "error:  0.27349999999999997\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "32\n",
      "activation: relu\n",
      "{}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.17528469665000254  minutes to train\n",
      "error:  0.08879999999999999\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "128\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.2588585240166746  minutes to train\n",
      "error:  0.08360000000000001\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "64\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.22971529146664882  minutes to train\n",
      "error:  0.394\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "16\n",
      "activation: relu\n",
      "{}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.32132372348332866  minutes to train\n",
      "error:  0.08050000000000002\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "32\n",
      "activation: relu\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: rmsprop\n",
      "Finished Training it took  0.32390604871670803  minutes to train\n",
      "error:  0.06599999999999995\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "64\n",
      "activation: relu\n",
      "{'0': ['1', 'out'], '1': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.28835017621665127  minutes to train\n",
      "error:  0.04749999999999999\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "32\n",
      "activation: tanh\n",
      "{'0': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.3085627777333381  minutes to train\n",
      "error:  0.06479999999999997\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "256\n",
      "activation: relu\n",
      "{}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.2631898257333281  minutes to train\n",
      "error:  0.09030000000000005\n",
      "\n",
      "\n",
      "batch size is a pos int, it should work\n",
      "128\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.2700880362500053  minutes to train\n",
      "error:  0.08530000000000004\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,1,0,0,16,2,0.27349999999999997,12127\n",
      "4,0,0,1,32,1,0.08879999999999999,24005\n",
      "64,0,1,0,128,1,0.08360000000000001,41065\n",
      "4,2,0,0,64,1,0.394,56304\n",
      "8,0,0,1,16,1,0.08050000000000002,77015\n",
      "64,2,0,1,32,2,0.06599999999999995,97980\n",
      "64,2,1,1,64,0,0.04749999999999999,116727\n",
      "32,1,1,0,32,1,0.06479999999999997,136705\n",
      "4,0,1,1,256,0,0.09030000000000005,153989\n",
      "8,0,0,0,128,0,0.08530000000000004,171753\n",
      "\n",
      "\n",
      "End of doe phase, the number of new configuration runs is: 10\n",
      "\n",
      "Starting optimization iteration 1\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: relu\n",
      "{'0': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.33705104089998106  minutes to train\n",
      "error:  0.06469999999999998\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "16,1,0,1,16,0,0.06469999999999998,194172\n",
      "\n",
      "Starting optimization iteration 2\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.17934454514997925  minutes to train\n",
      "error:  0.09499999999999997\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "16,2,0,0,64,0,0.09499999999999997,207043\n",
      "\n",
      "Starting optimization iteration 3\n",
      "batch size is a pos int, it should work\n",
      "128\n",
      "activation: relu\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.29269471476666997  minutes to train\n",
      "error:  0.044300000000000006\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,1,128,1,0.044300000000000006,226696\n",
      "\n",
      "Starting optimization iteration 4\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: relu\n",
      "{'0': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.14688858961665877  minutes to train\n",
      "error:  0.2388\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,1,0,1,16,0,0.2388,237375\n",
      "\n",
      "Starting optimization iteration 5\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: tanh\n",
      "{'0': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.32541488836668575  minutes to train\n",
      "error:  0.03739999999999999\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,1,0,0,16,0,0.03739999999999999,258886\n",
      "\n",
      "Starting optimization iteration 6\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: tanh\n",
      "{'0': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.22084170386666907  minutes to train\n",
      "error:  0.04179999999999995\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,1,0,0,16,1,0.04179999999999995,274196\n",
      "\n",
      "Starting optimization iteration 7\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: tanh\n",
      "{'0': ['out']}\n",
      "optimizer: rmsprop\n",
      "Finished Training it took  0.2789901592666865  minutes to train\n",
      "error:  0.16059999999999997\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,1,0,0,16,2,0.16059999999999997,293056\n",
      "\n",
      "Starting optimization iteration 8\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: relu\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.30028927346666023  minutes to train\n",
      "error:  0.04469999999999996\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,1,64,0,0.04469999999999996,313290\n",
      "\n",
      "Starting optimization iteration 9\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "256\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.23527375168335615  minutes to train\n",
      "error:  0.09519999999999995\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,1,0,256,0,0.09519999999999995,329541\n",
      "\n",
      "Starting optimization iteration 10\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.3605089662500177  minutes to train\n",
      "error:  0.03490000000000004\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,0,16,0,0.03490000000000004,353238\n",
      "\n",
      "Starting optimization iteration 11\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.4458280920166847  minutes to train\n",
      "error:  0.04400000000000004\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,0,16,1,0.04400000000000004,382162\n",
      "\n",
      "Starting optimization iteration 12\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "128\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.21624161784996734  minutes to train\n",
      "error:  0.08379999999999999\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,1,0,128,0,0.08379999999999999,397343\n",
      "\n",
      "Starting optimization iteration 13\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: relu\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.31540515653329443  minutes to train\n",
      "error:  0.045599999999999974\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,1,64,1,0.045599999999999974,418530\n",
      "\n",
      "Starting optimization iteration 14\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.21589252451667562  minutes to train\n",
      "error:  0.045599999999999974\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,0,64,0,0.045599999999999974,433709\n",
      "\n",
      "Starting optimization iteration 15\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "128\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.2543180270500064  minutes to train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.08409999999999995\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,0,0,128,0,0.08409999999999995,451059\n",
      "\n",
      "Starting optimization iteration 16\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: relu\n",
      "{'0': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.3389242710166703  minutes to train\n",
      "error:  0.03049999999999997\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,1,0,1,16,0,0.03049999999999997,473477\n",
      "\n",
      "Starting optimization iteration 17\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "128\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.27693554519998237  minutes to train\n",
      "error:  0.08179999999999998\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,0,0,128,1,0.08179999999999998,492140\n",
      "\n",
      "Starting optimization iteration 18\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.3490470667333284  minutes to train\n",
      "error:  0.08720000000000006\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,0,0,16,1,0.08720000000000006,515377\n",
      "\n",
      "Starting optimization iteration 19\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{'0': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.2665572811500169  minutes to train\n",
      "error:  0.05020000000000002\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,1,0,0,64,0,0.05020000000000002,533543\n",
      "\n",
      "Starting optimization iteration 20\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.2138269333166439  minutes to train\n",
      "error:  0.03949999999999998\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,0,64,1,0.03949999999999998,548544\n",
      "\n",
      "Starting optimization iteration 21\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: tanh\n",
      "{'0': ['1', 'out'], '1': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.36525672268332227  minutes to train\n",
      "error:  0.123\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "8,2,1,0,16,0,0.123,572884\n",
      "\n",
      "Starting optimization iteration 22\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "32\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.2798118012166621  minutes to train\n",
      "error:  0.09319999999999995\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,0,0,32,1,0.09319999999999995,591910\n",
      "\n",
      "Starting optimization iteration 23\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "32\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.3185047390166801  minutes to train\n",
      "error:  0.14090000000000003\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "8,2,0,0,32,1,0.14090000000000003,613097\n",
      "\n",
      "Starting optimization iteration 24\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "32\n",
      "activation: relu\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.33900863958333505  minutes to train\n",
      "error:  0.07320000000000004\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "16,2,0,1,32,1,0.07320000000000004,635497\n",
      "\n",
      "Starting optimization iteration 25\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.3837236598333523  minutes to train\n",
      "error:  0.08350000000000002\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "16,2,0,0,16,1,0.08350000000000002,660588\n",
      "\n",
      "Starting optimization iteration 26\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.27976445720002324  minutes to train\n",
      "error:  0.08360000000000001\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,0,0,64,1,0.08360000000000001,679390\n",
      "\n",
      "Starting optimization iteration 27\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "128\n",
      "activation: relu\n",
      "{}\n",
      "optimizer: rmsprop\n",
      "Finished Training it took  0.2782604510166493  minutes to train\n",
      "error:  0.12639999999999996\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,0,0,1,128,2,0.12639999999999996,698191\n",
      "\n",
      "Starting optimization iteration 28\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.2890217550667027  minutes to train\n",
      "error:  0.08440000000000003\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,1,0,64,1,0.08440000000000003,717715\n",
      "\n",
      "Starting optimization iteration 29\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "128\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.18218025805005406  minutes to train\n",
      "error:  0.042100000000000026\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,0,128,1,0.042100000000000026,730754\n",
      "\n",
      "Starting optimization iteration 30\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{'0': ['1', 'out'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.31908878463333773  minutes to train\n",
      "error:  0.04300000000000004\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,1,0,64,1,0.04300000000000004,752078\n",
      "\n",
      "Starting optimization iteration 31\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "128\n",
      "activation: relu\n",
      "{'0': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.26756597923328324  minutes to train\n",
      "error:  0.09999999999999998\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "8,1,0,1,128,1,0.09999999999999998,770381\n",
      "\n",
      "Starting optimization iteration 32\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "128\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.24698413370000102  minutes to train\n",
      "error:  0.13260000000000005\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "8,2,0,0,128,1,0.13260000000000005,787280\n",
      "\n",
      "Starting optimization iteration 33\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: relu\n",
      "{'0': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.313586799149986  minutes to train\n",
      "error:  0.039000000000000035\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,1,0,1,16,1,0.039000000000000035,808277\n",
      "\n",
      "Starting optimization iteration 34\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: relu\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.19028360790004323  minutes to train\n",
      "error:  0.10389999999999999\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "8,2,0,1,64,0,0.10389999999999999,821799\n",
      "\n",
      "Starting optimization iteration 35\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.2160499078000309  minutes to train\n",
      "error:  0.08330000000000004\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,0,0,0,64,0,0.08330000000000004,836863\n",
      "\n",
      "Starting optimization iteration 36\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "128\n",
      "activation: relu\n",
      "{'0': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.17298680631665775  minutes to train\n",
      "error:  0.2257\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "4,1,1,1,128,1,0.2257,849400\n",
      "\n",
      "Starting optimization iteration 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "32\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.35171062798332664  minutes to train\n",
      "error:  0.03959999999999997\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,2,0,0,32,1,0.03959999999999997,872687\n",
      "\n",
      "Starting optimization iteration 38\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: relu\n",
      "{'0': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.22608801458330466  minutes to train\n",
      "error:  0.0827\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "16,1,1,1,64,1,0.0827,888293\n",
      "\n",
      "Starting optimization iteration 39\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "64\n",
      "activation: tanh\n",
      "{'0': ['1'], '1': ['out']}\n",
      "optimizer: adam\n",
      "Finished Training it took  0.2561920993499977  minutes to train\n",
      "error:  0.06310000000000004\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "32,2,0,0,64,1,0.06310000000000004,905736\n",
      "\n",
      "Starting optimization iteration 40\n",
      "not a pos int!! type:  <class 'numpy.int64'>\n",
      "Now it should be int. type:  <class 'int'>\n",
      "16\n",
      "activation: relu\n",
      "{}\n",
      "optimizer: sgd\n",
      "Finished Training it took  0.3147137653166889  minutes to train\n",
      "error:  0.09740000000000004\n",
      "\n",
      "\n",
      "n_nodes,n_layers,skip,activation,batch_size,optimizer,Value,Timestamp\n",
      "128,0,0,1,16,0,0.09740000000000004,926785\n",
      "\n",
      "End of Random Scalarizations\n",
      "### End of the hypermapper script.\n",
      "this entire procedure took  15.447462448816623 minutes\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n",
      "best archtecture from BO:  n_layers           1.0000\n",
      "Value              0.0305\n",
      "optimizer          0.0000\n",
      "batch_size        16.0000\n",
      "n_nodes          128.0000\n",
      "Timestamp     473477.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "Name: 25, dtype: float64\n",
      "best archtecture from RS:  n_nodes          128.0000\n",
      "n_layers           2.0000\n",
      "skip               0.0000\n",
      "activation         1.0000\n",
      "batch_size        32.0000\n",
      "optimizer          1.0000\n",
      "Value              0.0343\n",
      "Timestamp     516519.0000\n",
      "Name: 20, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Took 14 minutes on my cpu to do 50 random samples and evaluate\n",
    "# Took 9 minutes on my cpu to do 10 random samples and then 20 Bayesian Optimization steps\n",
    "# Bayesian Optimization iteration takes about the same time as Random Search as search space is small\n",
    "\n",
    "# creates a problem scenario and saves it as example_mnist_scenario.json\n",
    "rs = 10\n",
    "bo = 40\n",
    "# op = \"stupid.csv\"\n",
    "op = 'mnist_BO_10+40.csv'\n",
    "create_scenario(rs, BO = bo, output_path = op)\n",
    "\n",
    "%cd\n",
    "%cd \"PycharmProjects/samuel_nas/my_hypermapper/scripts\"\n",
    "\n",
    "import hypermapper\n",
    "%cd \"..\"\n",
    "parameters_file = \"example_scenarios/quick_start/example_mnist_scenario.json\"\n",
    "# parameters_file = \"example_branin_scenario.json\"\n",
    "t_start = time.perf_counter()\n",
    "hypermapper.optimize(parameters_file, MNIST_function)\n",
    "print('this entire procedure took ', (time.perf_counter() - t_start) / 60, 'minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I have to make a nice way to sort the data since the randomly generated points are in reversed order in\n",
    "def make_scatter(data, color):\n",
    "    scores = np.zeros((len(data),2))\n",
    "\n",
    "    best = (-1, 1)\n",
    "    for i in range(len(data)):\n",
    "        value = data['Value'].iloc[i]\n",
    "        if value < best[1]:\n",
    "            best = (i, value)\n",
    "        scores[i, :] = [i, value]  # RS is saved reversed\n",
    "\n",
    "\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # view data\n",
    "    plt.figure(figsize=(10,4))\n",
    "\n",
    "    plt.scatter(scores[:,0], scores[:,1], color = color)\n",
    "    #plt.scatter(scores2[:,0], scores2[:,1], color = \"blue\")\n",
    "    #plt.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n",
    "    #plt.suptitle('Performance of the best found archtectiure as a function of samples. We trained on MNIST for 4 epochs')\n",
    "    #plt.xlabel('samples')\n",
    "    #plt.ylabel('fraction wrongly classified in validation set')\n",
    "    #plt.title('Regression Analysis')\n",
    "    #plt.text(1.0, 0, 'Loss = %.4f' % loss,\n",
    "                #fontdict={'size': 24, 'color':  'red'})\n",
    "    plt.show()\n",
    "    return best\n",
    "\n",
    "d_BO = pd.read_csv('mnist_BO_10+40.csv')     \n",
    "d_RS = pd.read_csv('mnist_RS_50.csv')    \n",
    "\n",
    "bestBO = make_scatter(d_BO, 'blue')\n",
    "bestRS = make_scatter(d_RS, 'orange')\n",
    "\n",
    "best_arch_BO = d_BO.iloc[bestBO[0]]\n",
    "best_arch_RS = d_RS.iloc[bestRS[0]]\n",
    "\n",
    "print('best archtecture from BO: ', best_arch_BO)\n",
    "print('best archtecture from RS: ', best_arch_RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv ('mnist_BO_10+40.csv')   \n",
    "data2 = pd.read_csv ('mnist_RS_50.csv')   \n",
    "\n",
    "scores = np.zeros((len(data),2))\n",
    "scores2 = np.zeros((len(data2),2))\n",
    "scores[0, :] = [0, data['Value'].iloc[0]]\n",
    "scores2[0, :] = [0, data2['Value'].iloc[0]]\n",
    "\n",
    "for i in range(1, len(data)):\n",
    "    scores[i, :] = [i, min(data['Value'].iloc[0:i])] # data is stored reversed\n",
    "    scores2[i, :] = [i, min(data2['Value'].iloc[0:i])]\n",
    "    \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# view data\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.scatter(scores[:,0], scores[:,1], color = \"orange\")\n",
    "plt.scatter(scores2[:,0], scores2[:,1], color = \"blue\")\n",
    "#plt.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n",
    "plt.suptitle('Performance of the best found archtectiure as a function of samples. We trained on MNIST for 1 epochs')\n",
    "plt.xlabel('samples')\n",
    "plt.ylabel('fraction wrongly classified in validation set')\n",
    "#plt.title('Regression Analysis')\n",
    "#plt.text(1.0, 0, 'Loss = %.4f' % loss,\n",
    "            #fontdict={'size': 24, 'color':  'red'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_layers           1.0000\n",
       "Value              0.0305\n",
       "optimizer          0.0000\n",
       "batch_size        16.0000\n",
       "n_nodes          128.0000\n",
       "Timestamp     473477.0000\n",
       "skip               0.0000\n",
       "activation         1.0000\n",
       "Name: 25, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_arch_BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_nodes          128.0000\n",
       "n_layers           2.0000\n",
       "skip               0.0000\n",
       "activation         1.0000\n",
       "batch_size        32.0000\n",
       "optimizer          1.0000\n",
       "Value              0.0343\n",
       "Timestamp     516519.0000\n",
       "Name: 20, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_arch_RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos int\n"
     ]
    }
   ],
   "source": [
    "a = 3**22 + 1\n",
    "if type(a) != int or a < 1:\n",
    "    print('not an pos int')\n",
    "else:\n",
    "    print('pos int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.array([1,2,3])\n",
    "a = False\n",
    "if type(u[0]) == 'numpy.int64':\n",
    "    a = True\n",
    "np.flip(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(int(u[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
