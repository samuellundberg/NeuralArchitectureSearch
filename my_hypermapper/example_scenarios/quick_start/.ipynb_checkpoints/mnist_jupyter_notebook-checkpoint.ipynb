{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make HyperMapper solve Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first just make two models that works on mnist. One fully connected an then one conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translates HyperMapper json to pyTorch module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in the nas notebook\n",
    "class json2pheno(nn.Module):\n",
    "    def __init__(self, json, nin, nout):\n",
    "        super(json2pheno, self).__init__()\n",
    "        # build layers from genome encoding\n",
    "\n",
    "        n_in = nin\n",
    "        fw_map = {}\n",
    "\n",
    "        # stupid loop but I want to know how deep the net will be\n",
    "        active_list = []\n",
    "        for param in json.keys():\n",
    "            if param[:-2] == 'active' and json[param] == 1:\n",
    "                active_list.append(param[-1])\n",
    "\n",
    "        # Add the active layers in the encoding to the model\n",
    "        for new_i, old_i in enumerate(active_list):\n",
    "            key = str(new_i)\n",
    "            setattr(self, key, nn.Linear(n_in, json['n_nodes']))\n",
    "\n",
    "            # We are on the last hidden layer, so we will not have any skipps here\n",
    "            if new_i == len(active_list) - 1:\n",
    "                fw_map[key] = ['out']\n",
    "                # at current setting n_in is same for all but first layer\n",
    "                n_in = json['n_nodes']\n",
    "                break\n",
    "\n",
    "            fw_map[key] = [str(new_i + 1)]\n",
    "\n",
    "            # Add skips to the fw_map. If they are to long, sent them to output layer\n",
    "            if 'skip_' + str(old_i) in json:\n",
    "                target = json['skip_' + str(old_i)] + new_i + 1\n",
    "                if target >= len(active_list):\n",
    "                    fw_map[key].append('out')\n",
    "                elif target > new_i + 1:\n",
    "                    fw_map[key].append(str(target))\n",
    "\n",
    "            # Again, this is same for all but first layer\n",
    "            n_in = json['n_nodes']\n",
    "\n",
    "        setattr(self, 'out', nn.Linear(n_in, nout))\n",
    "\n",
    "        # fw_scheme is a dict containing to which layers each layer is sending its output\n",
    "        # This will fail if we have non-forward connections\n",
    "        self.fw_scheme = fw_map\n",
    "        print(self.fw_scheme)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = 0\n",
    "        X = dict()\n",
    "        X[str(k)] = [x]\n",
    "        while hasattr(self, str(k)):\n",
    "            # pass trough all layers except the output layer\n",
    "            key = str(k)\n",
    "\n",
    "            # we might want to concat instead of sum, then we need to modify input_size in __init__\n",
    "            temp_x = sum(X[key])\n",
    "            temp_out = torch.tanh(getattr(self, key)(temp_x))\n",
    "            # this seem to work when doing the list thing with x\n",
    "            for target in self.fw_scheme[key]:\n",
    "                if target in X:\n",
    "                    X[target].append(temp_out)\n",
    "                else:\n",
    "                    X[target] = [temp_out]\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        # if k = 0 we have no active layers and a perceptron model\n",
    "        if k:\n",
    "            temp_x = sum(X['out'])\n",
    "        else:\n",
    "            temp_x = x\n",
    "\n",
    "        # Identity as output function since we do regression\n",
    "        # Add support for other types od problems problems\n",
    "        out = getattr(self, 'out')(temp_x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
